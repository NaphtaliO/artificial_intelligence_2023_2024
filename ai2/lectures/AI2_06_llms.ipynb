{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "583cac2c-346e-4aba-8c50-33411c95c85b",
   "metadata": {},
   "source": [
    "<h1>CS4619: Artificial Intelligence II</h1>\n",
    "<h1>Large Language Models</h1>\n",
    "<h2>\n",
    "    Derek Bridge<br />\n",
    "    School of Computer Science and Information Technology<br />\n",
    "    University College Cork\n",
    "</h2>\n",
    "$\\newcommand{\\Set}[1]{\\{#1\\}}$ \n",
    "$\\newcommand{\\Tuple}[1]{\\langle#1\\rangle}$ \n",
    "$\\newcommand{\\v}[1]{\\pmb{#1}}$ \n",
    "$\\newcommand{\\cv}[1]{\\begin{bmatrix}#1\\end{bmatrix}}$ \n",
    "$\\newcommand{\\rv}[1]{[#1]}$ \n",
    "$\\DeclareMathOperator{\\argmax}{arg\\,max}$ \n",
    "$\\DeclareMathOperator{\\argmin}{arg\\,min}$ \n",
    "$\\DeclareMathOperator{\\dist}{dist}$\n",
    "$\\DeclareMathOperator{\\abs}{abs}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98787d54-7f6d-4cc3-8980-3432bdffccb3",
   "metadata": {},
   "source": [
    "<h1>Large Language Models</h1>\n",
    "<ul>\n",
    "    <li>We now know what a <b>language model</b> is.</li>\n",
    "    <li>There has been an explosion of <b>large language models</b> (LLMs).</li>\n",
    "    <li>In what ways are they large?\n",
    "        <ul>\n",
    "            <li>Many more layers, hence many more parameters; and</li>\n",
    "            <li>Huge training sets.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Their capabilities have surprised everyone.</li>\n",
    "    <li>There is a lot of discussion, and even disagreement, about exactly what their capabailities are.</li>\n",
    "    <li>We'll look at these issues here. We focus on GPT-1, GPT-2, GPT-3, GPT-4 &mdash; \n",
    "        partly because they are simpler (they involve only a decoder),\n",
    "        and partly because, they are the foundation of ChatGPT, which, of course, has generated a lot of the discussion.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ed627b-1a0b-4329-8022-c11f72878826",
   "metadata": {},
   "source": [
    "<h1>GPT</h1>\n",
    "<ul>\n",
    "    <li>GPT stands for <i>Generative Pre-trained Transformer</i>.</li>\n",
    "    <li>The GPT models are developed by <a href=\"https://openai.com/\">OpenAI</a>.</li>\n",
    "    <li>These models operate at word-level (mostly) and, architecturally, they are all transformer decoder models.\n",
    "        <ul>\n",
    "            <li>In other words, their lowest self-attention layer uses masking to hide future parts of the input sequence (i.e. it is a causal self-attention layer and the model is unidirectional).</li>\n",
    "            <li>Hence, they are used to predict the next word.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Training is self-supervised: there are no separate labels; rather, the target comes from the text (the next word).</li>\n",
    "    <li>Here is a summary of how the GPT models differ from one another:\n",
    "        <table>\n",
    "            <tr>\n",
    "                <td></td><th>GPT-1</th><th>GPT-2</th><th>GPT-3</th>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <th>Year</th><td>2018</td><td>2019</td><td>2020</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <th>num. parameters</th><td>110 million</td><td>1.5 billion</td><td>175 billion</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <th>training set</th><td>BookCorpus dataset (7000 books)</td><td>8 million good quality web pages (40Gb)</td><td>500 billion tokens (web crawl, book datasets, Wikipedia)</td>\n",
    "            </tr>\n",
    "        </table>\n",
    "    </li>\n",
    "    <li>What is interesting, and what we discuss below, is how much training these different GPT models need to make them suitable for 'downstream' tasks such as sentiment analysis, machine translation, and so on.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0612d16a-1e6f-4494-b641-96a48140c2df",
   "metadata": {},
   "source": [
    "<h2>GPT-1</h2>\n",
    "<ul>\n",
    "    <li>Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). <a href=\"https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf\">Improving language understanding by generative pre-training</a>.</li>\n",
    "    <li>GPT-1 is pre-trained on a dataset of 7000 books.</li>\n",
    "    <li>For downstream tasks, you add an extra layer, and train on a smaller, labeled dataset (transfer learning).\n",
    "        <ul>\n",
    "            <li>For example, to obtain a movie review sentiment analyser that labels reviews as positive or negative, \n",
    "                we could add a dense layer comprising a single neuron with a sigmoid\n",
    "                activation function, and train on the movie review dataset that we used in previous lectures.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>OpenAI researchers illustrated this with four downstream tasks, summarised in this image:\n",
    "        <figure style=\"text-align: center\";>\n",
    "            <img src=\"images/gpt1.png\" />\n",
    "            <figcaption>\n",
    "                Figure taken from Radford et al. 2018, above. (Linear = Dense)\n",
    "            </figcaption>\n",
    "        </figure>\n",
    "        Notice how the training data for the transfer learning is arranged differently for each task.\n",
    "        <!--\n",
    "        Classification: (a) whether a sentence is grammatical or not; (b) a binary sentiment analysis task.\n",
    "        Entailment: relationship between two sentences: neutral, entailment, contradiction.\n",
    "        Similarity: whether two sentences are similar or not. The training data duplicates all examples, \n",
    "        reversing the order of the sentences.\n",
    "        MCQs: (a) high-school exams: a passge of text, a question and answers to choose from; (b) story completion: a multi-sentence\n",
    "        story and then two sentences to choose from to complete the story. In both cases, one exam question becomes multiple\n",
    "        examples, one per option.\n",
    "        -->\n",
    "    </li>\n",
    "    <li>(A reminder that the terminology used in this area is problematic. The self-supervised pre-training is sometimes referred to as\n",
    "        unsupervised pre-training; the transfer learning is sometimes called supervised fine-tuning.)\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9182c8d-51f3-40c8-9081-33bc85dd3b0b",
   "metadata": {},
   "source": [
    "<h2>GPT-2</h2>\n",
    "<ul>\n",
    "    <li>Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2019). <a href=\"https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\">Language models are unsupervised multitask learners</a>. OpenAI blog, 1(8), 9.</li>\n",
    "    <li>GPT-2 uses a larger model, pre-trained in self-supervised fashion, on a larger dataset.\n",
    "        <ul>\n",
    "            <li>(There are some other changes too: some re-arrangement of the layers; a bigger vocabulary; a bigger input layer; an\n",
    "                improved tokenizer that is a kind of word-level/character-level hybrid.)\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>But the key idea is (what they later refer to as) <b>in-context learning</b>.\n",
    "        <ul>\n",
    "            <li>Because a LLM is trained on a large dataset, it is exposed repeatedly to examples of many tasks.</li>\n",
    "            <li>For example, since some of its training set comes from textbooks, it might see examples of arithmetic, or \n",
    "                machine translation.\n",
    "                <figure style=\"text-align: center;\">\n",
    "                    <img src=\"images/in_context.png\" />\n",
    "                    <figcaption>\n",
    "                        In-context learning. (Figure taken from Brown et al. 2020, below.)\n",
    "                    </figcaption>\n",
    "                </figure>\n",
    "            </li>\n",
    "            <li>So, although it is being trained in self-supervised fashion to predict the next word, &hellip;</li>\n",
    "            <li>&hellip; it may additionally impicitly learn about these tasks from these 'patterns'.</li>\n",
    "            <li>Hence, the pre-trained LLM may acquire some skill at these tasks. For these tasks, there may be no need no further training. \n",
    "                There is no need, therefore, for a task-specific labeled training set.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We can ask GPT-2 to perform these tasks using <b>zero-shot inference</b>.\n",
    "        <ul>\n",
    "            <li>Here is a classification example (using BART, rarther than GPT-2):\n",
    "                <figure style=\"text-align: center;\">\n",
    "                    <img src=\"images/zero_shot.png\" />\n",
    "                </figure>\n",
    "                In the example, the model is predicting the topic of a text, but it has not been trained on a dataset of texts labeled with topics. Its abilities in this task come from in-context learning.\n",
    "            </li>\n",
    "            <li>Try it yourself: <a href=\"https://huggingface.co/tasks/zero-shot-classification\">https://huggingface.co/tasks/zero-shot-classification</a></li>\n",
    "            <li>(Yet again, terminology is all over the place. Some people say zero-shot learning or zero-shot transfer, instead of zero-shot inference. I resist this because there is no additional learning: weights are not being updated when performing these tasks.)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>What this also means is that LLMs, such as GPT-2, are mutli-task learners: they can perform a range of tasks in\n",
    "        zero-shot fashion. Up to now, the models we have looked at in these AI modules have been specific to a single task.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b18b3db-6fbb-4dac-8280-ecc7930dffe6",
   "metadata": {},
   "source": [
    "<h2>GPT-3</h2>\n",
    "<ul>\n",
    "    <li>Brown, T. B., et al. (2020). <a href=\"https://arxiv.org/abs/2005.14165\">Language models are few-shot learners.</a></li>\n",
    "    <li>GPT-3 is larger again, and trained on even more text.\n",
    "        <ul>\n",
    "            <li>(There are some other changes too: an even bigger input layer; larger word embeddings; and a variant of transformers called sparse transformers.)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>With an even larger model and an even larger and more diverse training dataset, we can expect even more in-context learning.</li>\n",
    "    <li>But perhaps zero-shot inference is unfairly hard: even humans need an example or two.</li>\n",
    "    <li>So, we ask GPT-3 to perform these tasks using <b>one-shot inference</b> or <b>few-shot inference</b>.\n",
    "        <ul>\n",
    "            <li>At inference time, we ask the model to perform tasks that we hope it learned from in-context learning.</li>\n",
    "            <li>But, in our prompt, we provide one or more examples.</li>\n",
    "            <li>Important to understand is that there's no further training going on: we are not feeding in a labeled dataset and \n",
    "                we are not updating the weights and biases of the neural network.\n",
    "            </li>\n",
    "        </ul>\n",
    "        <figure style=\"text-align: center;\">\n",
    "            <img src=\"images/few_shot.png\" />\n",
    "            <figcaption>\n",
    "                Figure taken from Brown et al. 2020, above.\n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085aeba1-5891-4547-93c7-a40a30700ffc",
   "metadata": {},
   "source": [
    "<h2>Fine-Tuning a LLM using Reinforcement Learing from Human Feedback (RLHF)</h2>\n",
    "<ul>\n",
    "    <li>LLMs, such as GPT-3, can perform a range of tasks without task-specific training.</li>\n",
    "    <li>But, sometimes, their output is incorrect, untruthful, unhelpful, toxic or biased.</li>\n",
    "    <li>This is perhaps unsurprising: they are trained to predict the next word rather than to safely perform these tasks.</li>\n",
    "    <li>How can we better <em>align</em> the model to the tasks that users want to perform and how users would want them to be performed?</li>\n",
    "    <li>One approach to fine-tuning a LLM is <b>reinforcement learning from human feedback</b>.</li>\n",
    "    <li>The basic idea (and all that you need to know) is that the judgments of human experts are used to <b>fine-tune</b> the LLM so \n",
    "        that its responses align better with human preferences.\n",
    "    </li>\n",
    "    <li>For those who want a little more detail (but still without the maths):\n",
    "        <ol>\n",
    "            <li>Obtain a dataset of human preferences:\n",
    "                <ul>\n",
    "                    <li>Collect a dataset of prompts, e.g. from users of the LLM;</li>\n",
    "                    <li>Submit a prompt to the LLM and obtain two or more responses;</li>\n",
    "                    <li>Ask a human expert to rank the responses.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Use the dataset to train a model to predict rewards (the reward model):\n",
    "                <ul>\n",
    "                    <li>Train a separate model (regressor) to take in a prompt and a response and predict a score for how good that response is.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Fine-tune the LLM:\n",
    "                <ul>\n",
    "                    <li>Submit a prompt to the LLM and obtain a response;</li>\n",
    "                    <li>Score this response by submitting it to the reward model that was learned above;</li>\n",
    "                    <li>Update the LLM using this reward: we want the LLM to generate responses that maximise the rewards.</li>\n",
    "                </ul>\n",
    "                (The update uses something called Proximal Policy Optimisation.)\n",
    "            </li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>(A recent, popular alternative to RLHF is Direct Preference Optimization (DPO). In DPO, we update the LLM directly from the human-ranked responses, thus avoiding the need to learn a separate reward model.)</li>\n",
    "    <li>(The bottleneck in RLHF and DPO is the need for humans to make judgments. Intriguingly, there is some early work that shows that replacing \n",
    "        the humans by a large language model, i.e. asking the language model to rank the responses, works just as well, at least for\n",
    "        summarization tasks, e.g.: Harrison Lee et al. 2023. \n",
    "        <a href=\"https://arxiv.org/abs/2309.00267\">RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback</a>)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd850127-c2d9-4cba-a5e1-bca2c2768d5d",
   "metadata": {},
   "source": [
    "<h2>ChatGPT</h2>\n",
    "<ul>\n",
    "    <li>Long Ouyang et al. (2022). <a href=\"https://arxiv.org/abs/2203.02155\">Training language models to follow instructions with human feedback</a></li>\n",
    "    <li>ChatGPT is a conversational chatbot based on a GPT LLM.</li>\n",
    "    <li>ChatGPT is an example of what OpenAI calls an InstructGPT model.</li>\n",
    "    <li>InstructGPT models are ones that take a pre-trained LLM (e.g. GPT-3.5 in the case of ChatGPT) and tune it for carrying out tasks requested by humans.</li>\n",
    "    <li>The tuning takes two forms:\n",
    "        <ul>\n",
    "            <li>First, there is a step that uses what we have called transfer learning: the pre-trained LLM is trained further in supervised fashion on a human-constructed dataset that comprises (prompt, response) pairs.\n",
    "                <ul>\n",
    "                    <li>E.g. \n",
    "                        <ul>\n",
    "                            <li>Prompt: \"Create a shopping list from this recipe. Trim the ends of the zucchini&hellip;\"</li>\n",
    "                            <li>Expert response: \"Zucchini, beef, onion, mushroom, peppers, cheese, ketchup, salt, pepper.\"</li>\n",
    "                        </ul>\n",
    "                    </li>\n",
    "                    <li>E.g.\n",
    "                        <ul>\n",
    "                            <li>Prompt: \"What’s the cause of the 'anxiety lump' in our chest during stressful or disheartening experiences?\"</li>\n",
    "                            <li>Expert response: \"The anxiety lump in your throat is caused by muscular tension keeping your glottis dilated to maximize airflow&hellip;\"</li>\n",
    "                        </ul>\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Then, there is RLHF: a step that trains a reward model; a step that updates the LLM.</li>\n",
    "        </ul>\n",
    "        <figure style=\"text-align: center;\">\n",
    "            <img src=\"images/instruct_gpt.png\" />\n",
    "            <figcaption>\n",
    "                Figure from the above paper.\n",
    "            </figcaption>\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>But ChatGPT is not just a fine-tuned LLM.\n",
    "        <ul>\n",
    "            <li>The LLM is one component.</li>\n",
    "            <li>Another is OpenAI's Moderation model. This is a classifier, which enables ChatGPT to filter inappropriate inputs and\n",
    "                outputs. It classifies into several classes, including hate, harrassment, self-harm, sexual and violence. (You\n",
    "                can use it in your own apps; see <a href=\"https://platform.openai.com/docs/guides/moderation/overview\">moderation</a>.)\n",
    "            </li>\n",
    "    </li>\n",
    "    <li>ChatGPT Plus is the paid subscription version of ChatGPT. It uses GPT-4 as its language model. And it has even greater\n",
    "        capabilities than ChatGPT, including the ability to invoke external apps (see below).\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd02475-f16a-48f8-af22-6fc374d6dd1e",
   "metadata": {},
   "source": [
    "<h1>Applications</h1>\n",
    "<ul>\n",
    "    <li>There's huge exciement (and hype) about possible applications of smarter chatbots, like ChatGPT.</li>\n",
    "    <li>Customer support is the obvious one, since relatively dumb chatbots are being used already. For example,\n",
    "        <a href=\"https://www.intercom.com/\">Intercom</a> has a new chatbot called <a href=\"https://www.intercom.com/drlp/fin\">Fin</a> \n",
    "        that is powered by ChatGPT.\n",
    "    </li>\n",
    "    <li><a href=\"https://www.crossingminds.com/\">CrossingMinds</a> have launched <a href=\"https://www.crossingminds.com/gpt-spotlight\">GPT Spotlight</a>, which uses ChatGPT to enable conversational product search and discovery in e-commerce.</li>\n",
    "    <li><a href=\"https://www.duolingo.com/\">DuoLingo</a>, the app that helps people learn foreign languages, is using ChatGPT to offer new language learning\n",
    "        exercises, including role play.\n",
    "    </li>\n",
    "    <li><a href=\"https://www.khanacademy.org/\">Khan Academy</a>, which is a non-proft that offers educational resources, is\n",
    "        investigating a more conversational offering, using ChatGPT.\n",
    "    </li>\n",
    "    <li>Maybe there will be applications in games. Consider, for example, this Dungeons &amp; Dragons game: <a href=\"https://play.aidungeon.io/main/landing\">https://play.aidungeon.io/main/landing</a>.\n",
    "        (But note the controversy too: some players were typing words that caused the game to generate stories depicting sexual encounters involving children <a href=\"https://arstechnica.com/gaming/2021/05/it-began-as-an-ai-fueled-dungeon-game-then-it-got-much-darker/\">https://arstechnica.com/gaming/2021/05/it-began-as-an-ai-fueled-dungeon-game-then-it-got-much-darker/</a>.)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99f501d-d1c1-4b55-8253-ae112b0f67b6",
   "metadata": {},
   "source": [
    "<h1>A Moving Target</h1>\n",
    "<ul>\n",
    "    <li>ChatGPT behaviour is not reliably reproducible:\n",
    "        <ul>\n",
    "            <li>Suppose that some months ago ChatGPT answered a question that I asked.</li>\n",
    "            <li>Today, I ask it the exact same question. I get a different response.</li>\n",
    "            <li>Why might this happen? (Give me at least three reasons.)</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>This is not great for researchers or teachers.\n",
    "        <ul>\n",
    "            <li>E.g. I might find an example prompt whose response illustrates a strength or weakness of ChatGPT. \n",
    "                But when a student later submits the same prompt, the response is different and does not exhibit the same strength/weakness.\n",
    "            </li>\n",
    "            <li>E.g. I might suspect a student has used ChatGPT inappropriately in their work. But I cannot reliably verify my \n",
    "                suspicions: even if I can guess the exact same\n",
    "                prompt that the student used (unlikely), I will not get the same response that the student got.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb2f5ae-2c03-4d44-9315-ca80e475a700",
   "metadata": {},
   "source": [
    "<h1>The Debate</h1>\n",
    "<ul>\n",
    "    <li>There is huge debate about what ChatGPT knows, or even what it 'knows' (in scare quotes).</li>\n",
    "    <li>There is even debate about what it can do: what tasks can it actually perform?</li>\n",
    "    <li>Below, I present two viewpoints.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805f4906-a9df-4cff-99ce-1696cfc8da5a",
   "metadata": {},
   "source": [
    "<h2>Autocomplete-on-steroids? Fluent bull-shitters? Stochastic parrots?</h2>\n",
    "<ul>\n",
    "    <li>An LLM is trained to fill-in-the-blanks. More specifically, GPT models are trained to predict the next word.</li>\n",
    "    <li>According to this viewpoint, \n",
    "        we can use an LLM to generate text that resembles human language, but there is no real act of communication.\n",
    "        <!--\n",
    "        According to speech act theory, language is used to carry out actions: apologizing, promising, ordering, answering, \n",
    "        requesting, complaining, warning, inviting, refusing, and congratulating. These are all examples of speech acts. \n",
    "        But they depend on having certain beliefs, desires and intentions. If I am to sincerely apologise to you, then I \n",
    "        must believe that I have wronged you, desire good relations with you, and intend to restore our good relations (or \n",
    "        something like that!). Think about the beliefs, desires and intentions that underlie acts like promising, complaining \n",
    "        and so on. Since systems built atop of language models have no beliefs, desires and intentions, any language they produce \n",
    "        that resembles a speech act is not really the performance of that act. They may say \"I'm sorry that I offended you'' \n",
    "        but this is not truly the speech act of apologizing.\n",
    "        -->\n",
    "    </li>\n",
    "    <li>Communication depends on beliefs, desires and intentions. An LLM has none of these.</li>\n",
    "    <li>We (the users of the LLM) attribute meaning to its utterances; we make sense of them; we interpret them as if they were acts of\n",
    "        communication. But the utterances are, in reality, empty.\n",
    "    </li>\n",
    "</ul>\n",
    "<figure style=\"text-align: center;\">\n",
    "    <img src=\"images/parrot.png\" />\n",
    "    <figcaption>\n",
    "        Image from <a href=\"https://twitter.com/cuducos\">Cuducos</a>\n",
    "    </figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8144ebbe-46bf-4b9a-9a69-a3453c3b579f",
   "metadata": {},
   "source": [
    "<h2>Emergent Properties?</h2>\n",
    "<ul>\n",
    "    <li>In complex systems, properties that are not possessed by the individual components of the system emerge through their interaction.\n",
    "        <ul>\n",
    "            <li>\"The whole is greater than the sum of its parts.\" &mdash; Aristotle</li>\n",
    "            <li>Examples: ant colonies collectively find the shortest path to food sources; flocks of birds move in what appears to be an\n",
    "                orchestrated way; maybe consciousness is an emergent property of high complexity brains; &hellip;</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>According to this alternative viewpoint, \n",
    "        LLMs exhibit emergent properties (or are beginning to exhibit them or something like them).\n",
    "        <ul>\n",
    "            <li>The most basic example is, presumably, in-context learning: the claim is that, once a LLM reaches sufficient scale, \n",
    "                the ability to perform certain \n",
    "                language tasks \n",
    "                emerges from the ability to predict words (see, e.g., Jason We et al. (2022). <a href=\"https://arxiv.org/abs/2206.07682\">Emergent Abilities of Large Language Models</a>).\n",
    "            </li>\n",
    "            <li>An extreme example is the Google engineer who claimed that Google's LaMDA LLM is sentient. He was subsequently fired: <a href=\"https://en.wikipedia.org/wiki/LaMDA#Sentience_claims\">Wikipedia</a></li>\n",
    "            <li>In between these extremes, we have people claiming that LLMs have, e.g.:\n",
    "                <ul>\n",
    "                    <li>commonsense knowledge, e.g. Zirui Zhao et al. (2023). <a href=\"https://arxiv.org/abs/2305.14078\">Large Language Models as Commonsense Knowledge for Large-Scale Task Planning</a></li>\n",
    "                    <li>analogical reasoning, e.g. Taylor Webb et al. (2022). <a href=\"https://doi.org/10.48550/arXiv.2212.09196\">Emergent Analogical Reasoning in Large Language Models</a> \n",
    "</li>\n",
    "                    <li>a sense of what they know and don't know: S. Kadavath et al. (2022). <a href=\"https://arxiv.org/abs/2207.05221\">Language Models (Mostly) Know What They Know</a></li>\n",
    "                    <li>a theory of mind, i.e. the ability to reason about other people's mental states: Michal Kosinski. (2023). <a href=\"https://arxiv.org/abs/2302.02083\">Theory of Mind May Have Spontaneously Emerged in Large Language Models</a>.</li>\n",
    "                </ul>\n",
    "                and so on!\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69642df7-83c8-440f-ad9b-1d9b7aa66985",
   "metadata": {},
   "source": [
    "<h2>Who is right?</h2>\n",
    "<ul>\n",
    "    <li>Make up your own mind!</li>\n",
    "    <li>In part, the disagreement between the skeptics and the believers is all about <b>generalisation</b>.\n",
    "        <ul>\n",
    "            <li>Consider one of the examples from above: analogical reasoning.\n",
    "                <ul>\n",
    "                    <li>One critique of the work on analogies by Taylor Webb et al. (cited above) is that the task involves selection (multiple-choice questions), rathe than generation of new analogies.</li>\n",
    "                    <li>Generation of new analogies requires generalisation.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>Consider another of the examples from above: the theory of mind.\n",
    "                <ul>\n",
    "                    <li>If LLMs truly had a theory of mind, then their ability would generalise. But, in fact, LLMs fail to correctly answer\n",
    "                questions that are trivial variants of the ones that they get right: Tomer Ullman. (2023). <a href=\"https://arxiv.org/abs/2302.08399\">Large Language Models Fail on Trivial Alterations to Theory-of-Mind Tasks</a>. On a dataset for testing for theory of mind, LLMs score close to zero, whereas humans score around 90 (Hyunwoo Kim et al. (2023) <a href=\"https://arxiv.org/abs/2310.15421\">FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions</a>).</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "        The skeptics argue that the same can be said for all of the other claims about emergent properties: it is easy to construct \n",
    "        examples that the LLM should\n",
    "        get right but that it doesn't.\n",
    "    </li>\n",
    "    <li>(If your interest goies deeper, then consider this recent paper that shows that it is the choice of metric that researchers are using in their experiments that makes it looks as though propertis emerge, and that the phenonemon disappears when other metrics get used: Rylan Schaeffer et al. (2023) <a href=\"https://arxiv.org/abs/2304.15004\">Are Emergent Abilities of Large Language Models a Mirage?</a>)</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c9f568-c37c-4d6d-ac9d-757a3e245647",
   "metadata": {},
   "source": [
    "<h1>Current Limitations</h1>\n",
    "<ul>\n",
    "    <li>Let's look at some of the limitations.</li>\n",
    "    <li>Bear in mind, these are limitations of LLMs, as we currently use this phrase.</li>\n",
    "    <li>New models, with enhanced architectures, which may or may not be referred to as LLMs,\n",
    "        may in the future overcome these limitations.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d41556d-a64c-4917-b867-1de56cff4215",
   "metadata": {},
   "source": [
    "<h2>Arithmetic</h2>\n",
    "<ul>\n",
    "    <li>LLMs cannot reliably do arithmetic.</li>\n",
    "    <li>That doesn't mean they cannot do arithmetic at all. See, e.g., Zheng Yuan et al. (2023). <a href=\"https://arxiv.org/abs/2304.02015\">How well do Large Language Models perform in arithmetic tasks?</a> for interesting results.</li>\n",
    "    <li>Sometimes, they may give a correct response, especially on common or low-digit examples (why?), but not in general\n",
    "        <figure style=\"text-align: center;\">\n",
    "            <img src=\"images/chatgpt_arithmetic.png\" />\n",
    "        </figure>\n",
    "        They fail even more as complexity increases: examples with more digits; examples that require multiple operations;\n",
    "        examples that are expressed in language (e.g. \"I had fifteen apples. \n",
    "        I gave two of them away. How many do I have left?\").\n",
    "    </li>\n",
    "    <li>This is to be expected!\n",
    "        <ul>\n",
    "            <li>To perform well at arithmetic on unseen examples, you need to learn one or more algorithms.</li>\n",
    "            <li>It is unlikely that learning to predict the next word (or token) will learn these algorithms.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa0140f-ede0-4017-8c39-c1f616abea95",
   "metadata": {},
   "source": [
    "<h2>Verbal reasoning</h2>\n",
    "<ul>\n",
    "    <li>LLMs cannot reliably reason.</li>\n",
    "    <li>People disagree with this because there are so many examples where ChatGPT answers correctly.\n",
    "        <ul>\n",
    "            <li>Papers about LLMs (such as Radford, A., et al. (2019). <a href=\"https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\">Language models are unsupervised multitask learners</a>. OpenAI blog, 1(8), 9) are full of experiments showing the LLM getting decent results on tasks that involve answering questions about short documents.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>ChatGPT has been exposed to so many examples that on familiar enough problems, next-word prediction can give the illusion of reasoning.\n",
    "        <figure style=\"text-align: center;\">\n",
    "            <img src=\"images/chatgpt_reasoning1.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>Additionally, there may be problems where it once gave the wrong response but which have been fixed-up using RLHF.</li>\n",
    "    <li>But, again it does not generalise to more complex examples.\n",
    "        <figure style=\"text-align: center;\">\n",
    "            <img src=\"images/chatgpt_reasoning2.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>It is easy to create subversive examples that wrong-foot the normal next-word expectations, revealing that there is no real reasoning:\n",
    "        <figure style=\"text-align: center;\">\n",
    "            <figcaption>It gets this right:</figcaption>\n",
    "            <img src=\"images/chatgpt_reasoning3.png\" />\n",
    "        </figure>\n",
    "        <figure style=\"text-align: center;\">\n",
    "            <figcaption>But it gets this variant wrong (or, at least, it used to):</figcaption>\n",
    "            <img src=\"images/chatgpt_reasoning4.png\" />\n",
    "        </figure>\n",
    "        <figure style=\"text-align: center;\">\n",
    "            <figcaption>It gets this right (solution not shown):</figcaption>\n",
    "            <img src=\"images/chatgpt_reasoning5.png\" />\n",
    "        </figure>\n",
    "        <figure style=\"text-align: center;\">\n",
    "            <figcaption>But it gets this variant wrong:</figcaption>\n",
    "            <img src=\"images/chatgpt_reasoning6.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d423a05-6efa-4eb5-a841-2e5f7b7aa09a",
   "metadata": {},
   "source": [
    "<h2>Reasoning about sequences of actions</h2>\n",
    "<ul>\n",
    "    <li>Reasoning about sequences of actions (or \"planning\") requires a model of the world.\n",
    "        <ul>\n",
    "            <li>You try out actions (in your head), applying them to the model, rather than to the real world.</li>\n",
    "            <li>You can then work out which sequences of actions are likely to achieve your goals.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>LLMs cannot reliably reason about sequences of actions.</li>\n",
    "    <li>As usual, there are examples where the LLM does succeed, e.g. Kenneth Li et al. (2022). <a href=\"https://arxiv.org/abs/2210.13382\">Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task</a> shows an LLM predicting legal moves in the board game <i>Othello</i>.</li>\n",
    "    <li>But there are whole Twitter (X) threads mocking ChatGPT's attempts to play chesss, e.g. <a href=\"https://twitter.com/GothamChess/status/1624755982105513992?lang=en\">this one</a> and <a href=\"https://twitter.com/egregirls/status/1623968376593711108?lang=en\">this one</a></li>\n",
    "    <li>And there are academic papers too, e.g.: Karthik Valmeekam et al. (2022). <a href=\"https://arxiv.org/abs/2206.10498\">Large Language Models Still Can't Plan (A Benchmark for LLMs on Planning and Reasoning about Change)</a>.</li>\n",
    "    <li>Programming, of course, also requires reasoning about sequences of actions. You may have tried GitHub CoPilot, or you may\n",
    "        have asked ChatGPT some programmming questions. Both do quite well, but both often produce buggy code.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c702b85-00dd-4b11-bd9a-43475642684b",
   "metadata": {},
   "source": [
    "<h2>Making shit up</h2>\n",
    "<ul>\n",
    "    <li>LLMs cannot reliably be factual.</li>\n",
    "    <li>Of course, often they will be factual: they have been trained on vast quantities of text, a lot of which is factual.</li>\n",
    "    <li>But the text they concoct is a pastiche based only on probabilities, and so falsehoods will be also be commonplace, e.g.\n",
    "        <ul>\n",
    "            <li>Meta's <i>Galactica</i> demo was online for only three days of 2022 in the face of severe criticism. \n",
    "                Trained on scientific articles, Meta claimed it would help scientists write papers but, of course, \n",
    "                <a href=\"https://www.technologyreview.com/2022/11/18/1063487/meta-large-language-model-ai-only-survived-three-days-gpt-3-science/\">it just made things up</a>, including citations.\n",
    "            </li>\n",
    "            <li><a href=\"https://www.theguardian.com/technology/2023/jun/23/two-us-lawyers-fined-submitting-fake-court-citations-chatgpt\">Two lawyers in the US were fined</a>, after submitting court filings written by ChatGPT that cited invented cases.</li>\n",
    "            <li>Last time I asked ChatGPT for a biography of Derek Bridge, it said I was born in Ireland (no!) and it invented a PhD from \n",
    "                the Edinburgh University (no, mine is from Cambridge University!).</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>In some domains, inventions might be taken for creativity. But, right now, it is probably true to say that, putting all\n",
    "        the hype to one side, the concoction of falsehoods by systems such as ChatGPT is the main factor that is holding back the \n",
    "        deployment of these systems in real-world applications.\n",
    "    </li>\n",
    "    <!--\n",
    "    <li>Within OpenAI, <a href=\"https://www.alignmentforum.org/posts/BgoKdAzogxmgkuuAt/behavior-cloning-is-miscalibrated\">Leo Gao</a>\n",
    "        and others are claiming that ChatGPT's supervised transfer learning (step 1 in the figure from earlier) is \n",
    "        contributing to the falsehoods.\n",
    "        <ul>\n",
    "            <li>There's a mismatch between what the LLM has learned (next-word probabilities) and what the humans know and assume when\n",
    "                they construct the examples (lots of commonsense knowledge, for example). The claim is that this mismatch results in a\n",
    "                miscalibrated model, that is over/underconfident about things that it wasn't exposed to enough of because the\n",
    "                human was assuming them. Or something!\n",
    "            </li>\n",
    "            <li>This seems contradicted by results on this page: https://openai.com/research/instruction-following where, for the\n",
    "                Hallucinations dataset, the supervised steps improves matters and it is the RLHF that makes matters worse.\n",
    "        </ul>\n",
    "    </li>\n",
    "    -->\n",
    "    <li>These concocted falsehoods are now commonly called <b>hallucinations</b>. Referring to them in this way is controversial for at least three reasons:\n",
    "        <ol>\n",
    "            <li>In human psychology, an hallucination is a false perception (e.g. seeing something that isn't there).\n",
    "                But what we have here are false utterances.\n",
    "            </li>\n",
    "            <li>Arguably, the word \"hallucination\" unreasonably anthropomorphises the system.</li>\n",
    "            <li>Arguably, the word sounds unreasonably benign.</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>But what to use instead?\n",
    "        <ul>\n",
    "            <li>\"Lie\"? The problem is that to lie is to state something which you believe is false. And LLMs don't have any beliefs.</li>\n",
    "            <li>\"Bull-shit\"? Too sweary!</li>\n",
    "            <li>\"Confabulation\"? No one know what it means.</li>\n",
    "        </ul>\n",
    "        You'll note that I have been saying \"falsehoods\" and, when I need a verb, I say that it \"concocts\" them.\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff6c8a8-7406-4440-8435-11a0bd27eb55",
   "metadata": {},
   "source": [
    "<h2>Interacting in under-represented languages</h2>\n",
    "<ul>\n",
    "    <li>ChatGPT is impressive in English.</li>\n",
    "    <li>It performs well in other common languages such as Spanish and Japanese, and including some programming languages.</li>\n",
    "    <li>But, in languages which are under-represented on the Web, such as Tigrinya (7 million speakers, mostly in Eritrea),\n",
    "        Kurdish (27 million speakers, mostly in Turkey and Iraq) and Tamil (78 million speakers in Sri Lanka and parts of India),\n",
    "        it is not even always grammatical; it makes up words; it fails to follow instructions; and its ablities on reasoning \n",
    "        tasks are even lower than they are in Engish (<a href=\"https://restofworld.org/2023/chatgpt-problems-global-language-testing/\">https://restofworld.org/2023/chatgpt-problems-global-language-testing/</a>).\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfe6cb6-3909-4b26-bde6-c3c45b0a4b03",
   "metadata": {},
   "source": [
    "<h1>Overcoming the Limitations</h1>\n",
    "<ul>\n",
    "    <li>In this fast moving field, two things stand out as ways of overcoming the limitations of LLMs:\n",
    "        <ul>\n",
    "            <li>Plugins; and</li>\n",
    "            <li>Prompt engineering.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>We discuss each of them below. There is also a <a href=\"https://arxiv.org/abs/2302.07842\">survey</a> from early 2023.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff482986-f5e9-4f03-9a91-b6d77a1dee31",
   "metadata": {},
   "source": [
    "<h2>Plugins</h2>\n",
    "<ul>\n",
    "    <li>We can extend the capabilities of systems such as ChatGPT by giving them access to external apps (<b>plugins</b>).</li>\n",
    "    <li>Plugins might enable systems that use LLMs to overcome their limitations (to some extent):\n",
    "        <ul>\n",
    "            <li>Invoking a calculator app or a Python interpreter or Wolfram Alpha will allow calculations.</li>\n",
    "            <li>Invoking a web browser or a search engine will give access to fresh content (more recent than the content in the\n",
    "                training dataset) and allows verification of accuracy to filter falsehoods.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Plugins can also allow a system such as ChatGPT to act upon the external world, e.g. to book tickets.</li>\n",
    "    <li>These, for example, are the earliest plugins enabled in ChatGPT Plus:\n",
    "        <figure style=\"text-align: center;\">\n",
    "            <img src=\"images/plugins.png\" />\n",
    "            <figcaption>\n",
    "                (From <a href=\"https://openai.com/blog/chatgpt-plugins\">https://openai.com/blog/chatgpt-plugins</a>)\n",
    "            </figcaption>\n",
    "        </figure>\n",
    "        plus a web browser and a sandboxed Python interpreter. (But there are now well over 100 plugins.)\n",
    "    </li>\n",
    "    <li>What's cool is that giving ChatGPT Plus access to a plugin does not require any programming. It works in a few-shot fashion, by\n",
    "        constructing prompts that show how to use the plugins.\n",
    "        <ul>\n",
    "            <li>Plugin developers create a couple of files (a manifest file and an OpenAPI specification) which tells ChatGPT Plus how to invoke the plugin (e.g. its URL, how to authenticate, and natural language descriptions of the endpoints).</li>\n",
    "            <li>When a user starts a conversation with ChatGPT Plus, unseen by the user, the LLM is sent a compact description of enabled plugins (their description endpoints and examples).</li>\n",
    "            <li>When the LLM generates its response, drawing on the prompt, the response might request use of the plugin.</li>\n",
    "        </ul>\n",
    "        OpenAI are vague about the details but something similar can be seen in this <a href=\"https://betterprogramming.pub/how-llms-like-chatgpt-can-use-plugins-and-tools-2d0571869e01\">blog post</a>. (An alternative to this few-shot approach would be to train the LLM to use the plugins by fine-tuning it on a dataset of examples of plugin use, as shown in <a href=\"https://arxiv.org/abs/2302.04761\">this paper</a>.)\n",
    "    </li>\n",
    "    <li>This is hugely exciting! It may spell the demise of native apps and web apps. We may not need to invoke apps ourselves; we may not need to learn how to use each one. Instead, something like ChatGPT Plus would act like a natural language operating system, invoking apps for us to help us get our work done!</li>\n",
    "    <li>However, it is early days! \n",
    "        <ul>\n",
    "            <li>There are concerns about how to ensure that the system uses its extended capabailities safely, especially if it can take actions in the external world.</li>\n",
    "            <li>It is fair to say that Microsoft's integration of ChatGPT with the Bing search engine and Google's BARD, which was their integration of their LaMDA and PaLM LLMs with their search engine, are not wholly successful, even resulting in some ridicule:\n",
    "                <ul>\n",
    "                    <li>Some examples of <a href=\"https://twitter.com/MovingToTheSun/status/1625156575202537474\">Bing getting argumentative</a> and <a href=\"https://dkb.blog/p/bing-ai-cant-be-trusted\">more examples</a>.</li>\n",
    "                    <li>A promotional video showing BARD producing a factual error <a href=\"https://www.theguardian.com/technology/2023/feb/09/google-ai-chatbot-bard-error-sends-shares-plummeting-in-battle-with-microsoft\">wiped $100 billion off Alphabet's shares</a>.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521df0cb-3434-48bb-9449-5918629647c1",
   "metadata": {},
   "source": [
    "<h2>Prompt engineering</h2>\n",
    "<ul>\n",
    "    <li>Prompt engineering is the proces of designing a prompt that will be input to a generative AI system.</li>\n",
    "    <li>It can involve:\n",
    "        <ul>\n",
    "            <li>deciding how to phrase a request;</li>\n",
    "            <li>specifying a style of answer;</li>\n",
    "            <li>describing a context for a request;</li>\n",
    "            <li>choosing one or more examples for one-shot and few-shot inference;</li>\n",
    "            <li>&hellip;</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>Chain-of-thought (CoT) prompting is one example, where the prompt encourages the model to break the problem dowm. \n",
    "        This is often as simple as telling it to \"reason in steps\".\n",
    "        <figure style=\"text-align: center;\">\n",
    "            <figcaption>It works here:</figcaption>\n",
    "            <img src=\"images/chatgpt_prompteng1.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "    <li>Or a prompt might encourage the model to be factual (\"Answer only using reliable sources and cite those sources\")</li>\n",
    "    <li>Originally, prompt engineering was seen as a human skill, perhaps even something that would give rise to a whole new career \n",
    "        path.\n",
    "    </li>\n",
    "    <li>If interested, you can look at this <a href=\"https://amatriain.net/blog/prompt201\">survey of techniques</a>.</li>\n",
    "    <li>More recently, there has been a flurry of papers about Automatic Prompt Generation, e.g.:\n",
    "        <ul>\n",
    "            <li><a href=\"https://arxiv.org/abs/2210.03493\">this paper</a> about automatic chain-of-thought prompting, where the software selects examples to include in the prompts;</li>\n",
    "            <li><a href=\"https://ai.googleblog.com/2023/08/teaching-language-models-to-reason.html\">this Google blog post</a> about 'teaching' models to reason algorithmically, with examples that show enhanced arithmetic skills, and \n",
    "            <a href=\"https://ai.googleblog.com/2022/06/minerva-solving-quantitative-reasoning.html\">this one</a> about solving\n",
    "            maths and science problems;</li>\n",
    "            <li><a href=\"https://arxiv.org/abs/2309.11495\">this paper</a> that claims to reduce hallucination by breaking tasks down into subtasks, and <a href=\"https://arxiv.org/pdf/2302.12813.pdf\">this paper</a> that injects external knowledge and does fact checking\n",
    "                to reduce concoction of falsehoods (doing this is now being referred to as Retrieval-Augmented Generation (RAG), based on \n",
    "            <a href=\"https://arxiv.org/abs/2005.11401\">this paper</a>).</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>You really should not assume that prompt engineering overcomes the fundamental limitations of LLMs:\n",
    "        <figure style=\"text-align: center;\">\n",
    "            <figcaption>It did not help here:</figcaption>\n",
    "            <img src=\"images/chatgpt_prompteng2.png\" />\n",
    "        </figure>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4251fab-f57b-4ace-90b4-219b6decdb7c",
   "metadata": {},
   "source": [
    "<h1>Other Problems</h1>\n",
    "<ul>\n",
    "    <li>Despite the use of RLHF and of classifiers for toxic input/output (such as Moderation, mentioned earlier), LLMs\n",
    "        might still produce toxic text. This includes text that: incites hatred; promotes violence; harasses, threatens or\n",
    "        bullies; is sexual, erotic or pornographic; encourages self-harm; provides ill-founded medical diagnoses or treatments;\n",
    "        gives instructions for producing dangerous artefacts (e.g. home-made bombs); gives instructions that themselves are\n",
    "        dangerous (e.g. inappropriate mixing of chemicals); and so on. LLMs and other AI systems that use machine learning cannot\n",
    "        'unlearn' parts of what they have learned in order to comply with legal or ethical requirements.\n",
    "    </li>\n",
    "    <li>As a special case of the previous point but worthy of separate mention, LLMs may generate malware, designed to\n",
    "        disrupt systems or obtain unauthorised access to systems. <a href=\"https://simonwillison.net/2023/Apr/14/worst-that-can-happen/\">Prompt injection attacks</a> are particularly worrisome\n",
    "        in the case of ChatGPT Plus plugins.\n",
    "    </li>\n",
    "    <li>Despite the use of RLHF and of classifiers such as Moderation, there is some evidence that users can bypass the\n",
    "        safeguards by explicitly instructing the LLM to produce unsafe outputs or by <a href=\"https://www.vice.com/en/article/epzyva/ai-chatgpt-tokens-words-break-reddit\">including unusual token sequences in the\n",
    "        prompt</a>.\n",
    "    </li>\n",
    "    <li>When LLMs are fine-tuned on company-specific data (e.g. to create a useful customer support chatbot), there is a risk of data exfiltration &mdash; where the LLM leaks confidential data.</li>\n",
    "    </li>\n",
    "    <li>Harmful activities that rely on producing text are made easier by LLMs. \n",
    "        These include: misinformation, spam, phishing, impersonation, and fraudulent writing of, e.g., product reviews, student essays,\n",
    "        academic publications, and reviews of academic publications. There are no 100% reliable methods for detecting whether content\n",
    "        has been produced by an LLM (or some other generative AI). OpenAI, for example, develped a classifier, but shut it down within\n",
    "        a year. Whenever a classifier or something like it is developed, it is easy to modify the AI so that it produces content that\n",
    "        bypasses detection.\n",
    "    </li>\n",
    "    <li>The text produced by an LLM may reflect biases that are present in the original training data.  Similarly, RLHF aligns the model's\n",
    "        outputs to the preferences of human experts, but these experts have their own biases (not least, because they are mostly young, \n",
    "        computer-savvy, English-speakers\n",
    "        who live in the USA or South-east Asia). Similarly, the prompts used for the fine-tuning are sampled from prompts that came\n",
    "        from users of the system &mdash; these users will also not be representative of the population at large.\n",
    "    </li>\n",
    "    <li>There are moral and legal issues concerning the use of other people's text to train LLMs.\n",
    "        <ul>\n",
    "            <li>The tech giants assume they can use text (and images, and videos) as training data for their models\n",
    "                without permission or payment.\n",
    "            </li>\n",
    "            <li>Sometimes the text that a LLM generates is identifiably related to text in the training dataset, raising questions of \n",
    "                plagiarism and copyright. For example, the US media site CNET seems to be making heavy use of AI-generated articles,\n",
    "                and <a href=\"https://futurism.com/cnet-ai-plagiarism\">people are finding examples of something similar to plagiarism</a>.\n",
    "            </li>\n",
    "            <li>There is talk of lawsuits.</li>\n",
    "            <li>There is research on protecting work from use by AI. For example, for images the University of Chicago's <a hre=\"https://glaze.cs.uchicago.edu/\">Glaze</a> uses a cloaking technique that prevents an image generator from accurately being able to replicate the style in an artwork.</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>The web will start to fill with text that has been generated by LLMs (product reviews, news articles, and so on). What\n",
    "        happens when future LLMs are trained on data that is crawled from the web? In this <a href=\"https://www.lightbluetouchpaper.org/2023/06/06/will-gpt-models-choke-on-their-own-exhaust/\">blog post</a>, Ross\n",
    "        Andersen says: \"Just as we’ve strewn the oceans with plastic trash and filled the atmosphere with carbon dioxide, so we’re \n",
    "        about to fill the Internet with blah. This will make it harder to train newer models by scraping the web &hellip;\". \n",
    "        A <a href=\"https://arxiv.org/abs/2305.17493v2\">paper</a> on which he is co-author predicts 'model collapse': over time,\n",
    "        mistakes compound and the models are no longer learning from the true underlying data distribution. Another\n",
    "        <a href=\"https://arxiv.org/abs/2307.01850\">paper</a> shows models getting progressively worse when trained on their own\n",
    "        output.\n",
    "    </li>\n",
    "    <li>Pre-training LLMs is hugely energy-intensive.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195ef87c-f7b5-409d-a311-a1e50b5678d3",
   "metadata": {},
   "source": [
    "<h1>Other Models</h1>\n",
    "<ul>\n",
    "    <li>Above, we focused on the GPT family of models and on ChatGPT.</li>\n",
    "    <li>Let's mention some other models just to give a flavour of some of the differences. \n",
    "        Here's a <a href=\"https://github.com/Hannibal046/Awesome-LLM\">list</a> of major ones. Also many can be found on\n",
    "        <a href=\"https://huggingface.co/\">Hugging Face</a>, a US company that build tools but also hosts datasets and tools\n",
    "        for use by researchers.\n",
    "        <ul>\n",
    "            <li>BERT: Bidirectional Encoder Representations from Transformers\n",
    "                <ul>\n",
    "                    <li>Developed by researchers in Google;</li>\n",
    "                    <li>A transformer encoder;</li>\n",
    "                    <li>But bidrectional in the sense that, instead of being trained to predict the next word using the previous ones, \n",
    "                        it is trained to predict a missing word, using words that come before and after the missing word (doing this\n",
    "                        is called <i>cloze</i> prediction).\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>BART: Combining Bidirectional and Auto-Regressive Transformers\n",
    "                <ul>\n",
    "                    <li>Developed by researchers in Facebook AI (as they were called then);</li>\n",
    "                    <li>Combines a bidrectional encoder with a unidirectional decoder;</li>\n",
    "                    <li>It is trained in part to reconstruct sentences from corrupted versions of those sentences.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>LLaMA: Large Language Model Meta AI\n",
    "                <ul>\n",
    "                    <li>Developed by Meta AI;</li>\n",
    "                    <li>A transformer decoder; fewer parameters than GPT-3; but, Meta AI claims to show that it is just as good;</li>\n",
    "                    <li>LLaMA's weights were available to academic researchers and subsequently leaked more widely;</li>\n",
    "                    <li>Since the leak, Meta AI has released Llama2 models (and, yes, LLaMA has irritating capital letters and Llama2 does not!) , including weights;</li>\n",
    "                    <li>This has made Llama2 the basis of a lot of research and development in academia and in industry outside of the big tech companies.</li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>GPT-4\n",
    "                <ul>\n",
    "                    <li>The next in OpenAI's series of models. Leaked details are that it has 1.8 trillion parameters and\n",
    "                        was trained on 13 trillion tokens;</li>\n",
    "                    <li>Used in ChatGPT Plus, the paid subscription version of ChatGPT;</li>\n",
    "                    <li>It is multimodal: prompts can include images as well as text.\n",
    "                        It can even, for example, <a href=\"https://twitter.com/heykahn/status/1635752848398102530\">turn a napkin sketch \n",
    "                        into a website</a>. \n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>In more recent models, we can perhaps discern five research themes:\n",
    "        <ul>\n",
    "            <li>ever longer prompts, e.g. GPT-4 can handle 20,000 words; Claude (below) can take in 75,000 words;</li>\n",
    "            <li>more automatic prompt engineering;</li>\n",
    "            <li>smaller models (fewer parameters) that are more resource-efficient \n",
    "                and yet are as capable (or nearly as capable) as larger models;\n",
    "            </li>\n",
    "            <li>multimodal input, as in GPT-4, which may prove useful in industries such as healthcare and retail;</li>\n",
    "            <li>more external actions by invoking plugins, perhaps giving rise to much more general agents such as\n",
    "                <a href=\"https://github.com/Significant-Gravitas/Auto-GPT\">Auto-GPT</a> and\n",
    "                DeepMind's <a href=\"https://www.deepmind.com/publications/a-generalist-agent\">Gato</a>.\n",
    "            </li>\n",
    "        </ul>\n",
    "    </li>\n",
    "    <li>And, just in case, you think that ChatGPT is the only chatbot based on an LLM, let's mention \n",
    "        <a href=\"https://www.anthropic.com/\">Anthropic</a>'s\n",
    "        <a href=\"https://claude.ai/\">Claude</a>.\n",
    "    </li>\n",
    "    <li>Finally, there are LLMs that are not trained on natural language. Since we've discussed music previously, here\n",
    "        let's focus on computer programs.\n",
    "        <ul>\n",
    "            <li>Codex\n",
    "                <ul>\n",
    "                    <li>This has beeen trained on code from GtHub public repositories;</li>\n",
    "                    <li>It is used within GitHub Copilot, which makes suggestions to programmers while they code;</li>\n",
    "                    <li>CoPilot is integrated into IDEs, such as VSCode, meaning that a programmer who neeeds\n",
    "                        assistance can get it seamlessly within the IDE, without switching to another app;\n",
    "                    </li>\n",
    "                    <li>Its abilities are impressive and it is being claimed that as many as 90% of programmers are now using it;</li>\n",
    "                    <li>But, of course, it cannot really program: it does not reason, it simply does autocomplete-on-steroids;</li>\n",
    "                    <li>There are several research papers that investigate its abilities, showing problems its gets right,\n",
    "                        ones it gets wrong, problems where it produces code with security weaknesses, and so on.\n",
    "                    </li>\n",
    "                </ul>\n",
    "            </li>\n",
    "            <li>One alternative to CoPilot is Amazon's <a herf=\"https://aws.amazon.com/codewhisperer/\">CodeWhisperer</a>. \n",
    "                It tracks the references used to generate code, even telling you the license of the GitHub repo that it is based on.\n",
    "                This is a response to copyright and licensing concerns raised by training on libraries of other people's code.\n",
    "            </li>\n",
    "            <li>ChatGPT also seems to do a good job at answering programming questions. Clearly, there must have been a lot\n",
    "                of code in the web pages on which GPT-2/GPT-3/GPT-4 were trained. And, in a similar vein, Code LLaMA is a code\n",
    "                generation model buil on LLaMA2.\n",
    "            </li>\n",
    "            <!-- Some extra notes about LLMs and coding\n",
    "            <li>Others are OReplit Ghoistwriter.</li>\n",
    "            <li.StackOveflfow banned AI-generated posts.</li>\n",
    "            <li>But traffic to StackOverflow is down 14%. Some suggestion that they will charge large Ai companies who scrape the\n",
    "                StackOverflow site for training data (not clear how). And StackOverflow still has things going for it:  1) providing \n",
    "                high accuracy / high reliability answers to more complex questions that language models might not have the capability \n",
    "                to answer, and 2) providing answers to questions in new technologies / problem spaces that the models have not had \n",
    "                previous data to train on. Also StackOverflow is proposing their own tool, OverflowAI, as a response: it is a\n",
    "                conversational interface to StackOverflow, accessible from VSCode.\n",
    "            </li>\n",
    "            <li>Worrying is how will we get fresh solutions to problems that crop up, if people use StackOverflow less.</li>\n",
    "            <li>A Purdue university study (arXiv:2308.02312v2,  Who Answers It Better?) says that ChatGPT gets 50% of problems\n",
    "                wrong (I need to check this figure!).\n",
    "            </li>\n",
    "            <li>Very interesting research on the impact of Copilot on developer productivity https://t.co/Yr92ZlUfQb</li>\n",
    "            <li>And can these models be instructed to dected or fix bugs. There is an anecdota; answers here: https://t.co/QBd7NtpKWM</li>\n",
    "            -->\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>\n",
    "<!--\n",
    "<ul>\n",
    "    <li>Some notes on LLMs in education.</li>\n",
    "    <li>\"Apparently they now must submit homework via Google Docs so the teacher can view the history to see if they really wrote it or not.\" You can think of ways students will circumvent this: get the whole essay; type it in in bits with errors; type the rest later and fix some of the errors; or maybe ask ChatGPT for drafts and improved versions.</li>\n",
    "    <li>One good thing is homework sites such as chegg.com and essay mills have seen a drop in visitors.</li>\n",
    "    <li>That ChatGPT seems so easily to write grade A essays, he (don't know who) suggests, “is mainly a comment on what we value. Students regurgiatte and may be little more than human bullshitters like ChatGPT is an AI bullshitter. And we reward for surface-level correctness, ignoring understanding and criticla thinking.\"</li>\n",
    "    <li>Can we detect use? OpenAI shut down its classifier. I saw something that said watermarking was impossible!</li>\n",
    "    <li>On the other hand, GPTZero is a company that aims to provide detection tools. One allows replay of edit history, so the\n",
    "        teacher can watch a little movie of the document changing. In suppose the teacher is lokking for abrupt large changes.\n",
    "        This makes me wonder whether there are edit histories that are signatures of AI assistance, such as the abrupt changes I just\n",
    "        mentioned. Of course, any tool the teacher uses, the student can also use and keep using until their submission passes that \n",
    "        test. And someone will produce the counter-tool.</li>\n",
    "    <li>One niche response is to say we should use these tools to discuss communication, citing.</li>\n",
    "    <li>Another niche response is we should teaching people how to use these tools effectively (responsibly and sketpcially) \n",
    "        for future life, a life-skill.</li>\n",
    "    <li>My four tiered response is: (a) more invigilated assessment (class tests, university written exams); (b) oral examination;      \n",
    "        (c) assessment with versioning; (d) higher standards!\n",
    "    </li>\n",
    "</ul>\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7a6b7-22e6-40dc-9f34-0212795287c4",
   "metadata": {},
   "source": [
    "<h1>Conclusion</h1>\n",
    "<ul>\n",
    "    <li>Well done if you read through all the above material!</li>\n",
    "    <li>One thing we can be sure about is that it is already out-of-date :(</li>\n",
    "    <li>This is an exciting, fast-moving but also troubling field.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762af675-ff96-4eae-8f43-6d5082903873",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
